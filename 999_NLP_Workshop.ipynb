{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An NLP workshop - Categorizing tweets into relevant or non-relevant\n",
    "#### adapted from https://github.com/hundredblocks/concrete_NLP_tutorial.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Overview\n",
    "\n",
    "In this workshop you will combine a few different NLP techniques to classify tweets. You will:\n",
    "- clean up your input data\n",
    "- transform your text data into numerical vectors (because machine learning algorithms need numbers as inputs!)\n",
    "- use those vectors as input to machine learning classifiers\n",
    "\n",
    "We will present you with the code for three different **word embeddings** (i.e. ways to turn text into vectors of numbers) and two different **classification algorithms**. It will be your job to select an embedding and a model, train it with the data provided (you might want to perform some additional **cleansing** of the data beforehand) and then test the accuracy of your trained model. The team with the most accurate model wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dataset: Disasters on social media\n",
    "Contributors looked at over 10,000 tweets retrieved with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous). Thank you [Crowdflower](https://www.crowdflower.com/data-for-everyone/).\n",
    "\n",
    "### Why it matters\n",
    "We will try to correctly predict tweets that are about disasters. This is a very relevant problem, because:\n",
    "- It is actionable to anybody trying to get signal from noise (such as police departments in this case)\n",
    "- It is tricky because relying on keywords is harder than in most cases like spam\n",
    "- An analagous problem in finance is analyzing sentiment on news stories about companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the code\n",
    "First lets import all the libraries we will need upfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"socialmedia_relevant_cols.csv\", encoding='ISO-8859-1')\n",
    "questions.columns=['text', 'choose_one', 'class_label']\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing\n",
    "\n",
    "Let's create a function to clean up our data, and save it back to disk for future use.\n",
    "\n",
    "For now all we are going to do is to convert everything to lower case and remove URLs, but after examining the data you might want to add code to remove punctuation marks or other irrelevant characters or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"http\\S+\", \"\", elem))  # get rid of URLs\n",
    "    return df\n",
    "\n",
    "questions = standardize_text(questions, \"text\")\n",
    "\n",
    "questions.to_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = pd.read_csv(\"clean_data.csv\")\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "Let's look at our class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions.groupby(\"class_label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our classes are pretty balanced, with a slight oversampling of the \"Irrelevant\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data is clean, now it needs to be prepared\n",
    "Now that our inputs are more reasonable, let's transform our inputs in a way our model can understand. This implies:\n",
    "- Tokenizing sentences to a list of separate words\n",
    "- Creating a train test split\n",
    "- Inspecting our data a little more to validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "clean_questions[\"tokens\"] = clean_questions[\"text\"].apply(tokenizer.tokenize)\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting our dataset a little more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for tokens in clean_questions[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split\n",
    "First lets split our data into a training set and a test set. Later on we will 'fit' our model using the training set and then use the model to predict the labels for the test set and compare the predicted labels against the actual test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "list_corpus = clean_questions[\"text\"]\n",
    "list_labels = clean_questions[\"class_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=40)\n",
    "\n",
    "print(\"Training set: %d samples\" % len(X_train))\n",
    "print(\"Test set: %d samples\" % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to help us assess accuracy of our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define a function that plots a *Confusion Matrix* which helps us see our false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the Machine Learning\n",
    "Now that our data is clean and prepared, let's dive in to the machine learning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter embeddings\n",
    "Machine Learning on images can use raw pixels as inputs. Fraud detection algorithms can use customer features. What can NLP use?\n",
    " \n",
    "A natural way to represent text for computers is to encode each character individually, this seems quite inadequate to represent and understand language. Our goal is to first create a useful embedding for each sentence (or tweet) in our dataset, and then use these embeddings to accurately predict the relevant category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Counts\n",
    "The simplest approach we can start with is to use a bag of words model. A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "bow = dict()\n",
    "bow[\"train\"] = (count_vectorizer.fit_transform(X_train), y_train)\n",
    "bow[\"test\"]  = (count_vectorizer.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow[\"train\"][0].shape)\n",
    "print(bow[\"test\"][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Bag of Words\n",
    "Let's try a slightly more subtle approach. On top of our bag of words model, we use a TF-IDF (Term Frequency, Inverse Document Frequency) which means weighing words by how frequent they are in our dataset, discounting words that are too frequent, as they just add to the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "tfidf = dict()\n",
    "tfidf[\"train\"] = (tfidf_vectorizer.fit_transform(X_train), y_train)\n",
    "tfidf[\"test\"]  = (tfidf_vectorizer.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf[\"train\"][0].shape)\n",
    "print(tfidf[\"test\"][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Capturing semantic meaning\n",
    "Our first models have managed to pick up on high signal words. However, it is unlikely that we will have a training set containing all relevant words. To solve this problem, we need to capture the semantic meaning of words. Meaning we need to understand that words like 'good' and 'positive' are closer than apricot and 'continent'.\n",
    "\n",
    "Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"~/Downloads/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, clean_questions)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(embeddings, list_labels, \n",
    "                                                                    test_size=0.2, random_state=40)\n",
    "\n",
    "w2v = dict()\n",
    "w2v[\"train\"] = (X_train_w2v, y_train_w2v)\n",
    "w2v[\"test\"]  = (X_test_w2v, y_test_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression classifier\n",
    "Starting with a logistic regression is a good idea. It is simple, often gets the job done, and is easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', random_state=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine classifier\n",
    "Common alternative to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm_classifier = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = w2v                  # bow | tfidf | w2v\n",
    "classifier = lsvm_classifier     # lr_classifier | lsvm_classifier | nb_classifier\n",
    "\n",
    "classifier.fit(*embedding[\"train\"])\n",
    "\n",
    "y_predict = classifier.predict(embedding[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's start by looking at some metrics to see if our classifier performed well at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(embedding[\"test\"][1], y_predict)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(embedding[\"test\"][1], y_predict)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging text structure\n",
    "Our models have been performing well, but they completely ignore the structure. To see whether capturing some more sense of structure would help, we will try a final, more complex model.\n",
    "\n",
    "## CNNs for text classification\n",
    "Here, we will be using a Convolutional Neural Network for sentence classification. While not as popular as RNNs, they have been proven to get competitive results (sometimes beating the best models), and are very fast to train, making them a perfect choice for this workshop. First let's embed our text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tokenize the text, creating an index for every unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "VALIDATION_SPLIT=.2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(clean_questions[\"text\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(clean_questions[\"text\"].tolist())\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN needs a constant length input, so make each sequence have a maximum length of 35, padding where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the sequences and determine the train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(clean_questions[\"class_label\"]))\n",
    "indices = np.arange(cnn_data.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "cnn_data = cnn_data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every word in the word index, find the corresponding word2vec embedding, substituting a random embedding if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define a simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\n",
    "from tensorflow.keras.layers import SpatialDropout1D, Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, weights=[embeddings], input_length=max_sequence_length, trainable=trainable)) \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(labels_index, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(clean_questions[\"class_label\"].unique())), trainable=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the simpler classifiers used earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(x_val, batch_size=128)\n",
    "y_val_equiv = []\n",
    "for val in y_val:\n",
    "    if   val[0]== 1. : y_val_equiv.append(0)\n",
    "    elif val[1]== 1. : y_val_equiv.append(1)\n",
    "    else             : y_val_equiv.append(2)\n",
    "print(y_hat.shape)\n",
    "print('-------')\n",
    "print(len(y_val_equiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cnn = confusion_matrix(y_val_equiv, y_hat)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm_cnn, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlpw)",
   "language": "python",
   "name": "nlpw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
